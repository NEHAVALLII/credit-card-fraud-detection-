# -*- coding: utf-8 -*-
"""23099463.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W5264wu4giBW9jxuzDhaWGSwqOSyYtLX
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings('ignore')
sns.set(style="whitegrid")

from google.colab import files
uploaded = files.upload()

# read the uploaded CSV file
data = pd.read_csv("creditcard.csv")

# quick look at the first few rows
data.head()

# dataset shape
print("Shape:", data.shape)

data.info()

# check for missing values
print("Missing values:\n", data.isnull().sum())

# statistical summary
data.describe()

counts = data["Class"].value_counts()
sns.barplot(x=counts.index, y=counts.values, palette="viridis")
plt.title("Class Distribution (0=Non-Fraud, 1=Fraud)")
plt.xlabel("Transaction Class")
plt.ylabel("Count")
plt.show()
print("Number of Non-Fraud cases (Class 0):", counts[0])
print("Number of Fraud cases (Class 1):", counts[1])

corr = data.corr()["Class"].sort_values(ascending=False)
print("Top correlated features with fraud:\n", corr.head(10))

plt.figure(figsize=(10,6))
sns.heatmap(data.corr(), cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

# Transaction amount by class
sns.boxplot(x="Class", y="Amount", data=data, palette="Set2")
plt.title("Transaction Amount Distribution by Class")
plt.show()

# Distribution of transaction time
sns.histplot(data["Time"], bins=50, color="skyblue")
plt.title("Distribution of Transaction Time")
plt.show()

"""# Distribution of Transaction Amounts (Log Scale)

> Add blockquote


"""

plt.figure(figsize=(8,5))
sns.histplot(np.log1p(data['Amount']), bins=50, color='purple')
plt.title("Distribution of Log-Transformed Transaction Amounts")
plt.xlabel("Log(Transaction Amount + 1)")
plt.ylabel("Frequency")
plt.show()

"""# Compare Time vs. Amount by Class"""

plt.figure(figsize=(10,6))
sns.scatterplot(x='Time', y='Amount', hue='Class', data=data, alpha=0.6, palette={0:'skyblue', 1:'red'})
plt.title("Transaction Amount vs. Time (Fraud vs Non-Fraud)")
plt.xlabel("Time (seconds since first transaction)")
plt.ylabel("Transaction Amount")
plt.legend(["Non-Fraud", "Fraud"])
plt.show()

"""# Boxplots for Highly Correlated Features"""

top_features = ['V14', 'V12', 'V10']  # often highly correlated with fraud
plt.figure(figsize=(10,6))
for i, feature in enumerate(top_features, 1):
    plt.subplot(1, 3, i)
    sns.boxplot(x='Class', y=feature, data=data, palette="Set2")
    plt.title(f"{feature} vs Class")
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

"""# Take 350 Fraud + 350 Non-Fraud Transactions"""

fraud_df = data[data['Class'] == 1].sample(n=350, random_state=42)
non_fraud_df = data[data['Class'] == 0].sample(n=350, random_state=42)
balanced_df = pd.concat([fraud_df, non_fraud_df]).sample(frac=1, random_state=42)

print("\nBalanced dataset shape:", balanced_df.shape)
print(balanced_df['Class'].value_counts())

"""# Check correlation between V1–V28 and Class"""

corr = balanced_df.corr()['Class'][1:29]  # V1–V28 columns
print("\nCorrelation with Class:\n", corr)

# Drop columns with near-zero correlation
corr_threshold = 0.05  # you can adjust this
drop_features = corr[abs(corr) < corr_threshold].index
print("\nDropping low-correlation features:", list(drop_features))
balanced_df = balanced_df.drop(columns=drop_features)

"""Sampling and Feature Selection

"""

non_fraud_imb = data[data['Class'] == 0].sample(3000, random_state=42)
fraud_imb = data[data['Class'] == 1].sample(350, random_state=42)
df_imb = pd.concat([non_fraud_imb, fraud_imb]).sample(frac=1, random_state=42)

def feature_select(df):
    corr = df.corr()['Class'][1:]
    drop_features = corr[abs(corr) < 0.05].index
    return df.drop(columns=drop_features)

df_imb = feature_select(df_imb)
balanced_df = feature_select(balanced_df)

"""# Split features and target"""

# SPLIT DATASETS

# ---------------- IMBALANCED ----------------
X_imp = df_imb.drop("Class", axis=1)
y_imp = df_imb["Class"]
X_imp = StandardScaler().fit_transform(X_imp)
X_imp_train, X_imp_test, y_imp_train, y_imp_test = train_test_split(
    X_imp, y_imp, test_size=0.3, random_state=42, stratify=y_imp)

# ---------------- BALANCED ----------------
X_bal = balanced_df.drop("Class", axis=1)
y_bal = balanced_df["Class"]
X_bal = StandardScaler().fit_transform(X_bal)

"""# Train-Test Split"""

m = SMOTE(random_state=42)
X_bal_res, y_bal_res = m.fit_resample(X_bal, y_bal)

X_bal_train, X_bal_test, y_bal_train, y_bal_test = train_test_split(
    X_bal_res, y_bal_res, test_size=0.3, random_state=42, stratify=y_bal_res)

"""Artificial Neural Network (ANN)

"""

# artifical Neural Network (Adam Optimizer without SciKeras GridSearch)

def build_model(optimizer, input_dim):
    model = Sequential()
    model.add(Dense(32, activation='relu', input_dim=input_dim))
    model.add(Dropout(0.3))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# artifical Neural Network WITHOUT Optimizer (Default SGD)

model_no_opt = build_model(SGD(), X_imp_train.shape[1])
history_no_opt = model_no_opt.fit(
    X_imp_train, y_imp_train, epochs=15, batch_size=32,
    validation_split=0.2, verbose=0)
y_pred_no_opt = (model_no_opt.predict(X_imp_test) > 0.5).astype("int32")
no_opt_acc = accuracy_score(y_imp_test, y_pred_no_opt)
no_opt_precision = precision_score(y_imp_test, y_pred_no_opt)
no_opt_recall = recall_score(y_imp_test, y_pred_no_opt)
no_opt_f1 = f1_score(y_imp_test, y_pred_no_opt)

#  Neural Network WITH ADAM OPTIMIZER
model_adam = build_model(Adam(learning_rate=0.001), X_imp_train.shape[1])
history_adam = model_adam.fit(
    X_imp_train, y_imp_train, epochs=15, batch_size=32,
    validation_split=0.2, verbose=0)
y_pred_adam = (model_adam.predict(X_imp_test) > 0.5).astype("int32")
nn_acc = accuracy_score(y_imp_test, y_pred_adam)
nn_precision = precision_score(y_imp_test, y_pred_adam)
nn_recall = recall_score(y_imp_test, y_pred_adam)
nn_f1 = f1_score(y_imp_test, y_pred_adam)

# SAME MODEL WITH IMBALANCED VS BALANCED (SMOTE)

# Use Adam optimizer for both to show SMOTE improvement
#imbalace data with ann
model_imp = build_model(Adam(0.001), X_imp_train.shape[1])

model_imp.fit(X_imp_train, y_imp_train, epochs=15, batch_size=32, verbose=0)

pred_imp = (model_imp.predict(X_imp_test) > 0.5).astype("int32")
acc_imp = accuracy_score(y_imp_test, pred_imp)

#balace nn model
model_bal = build_model(Adam(0.001), X_bal_train.shape[1])
model_bal.fit(X_bal_train, y_bal_train, epochs=15, batch_size=32, verbose=0)

pred_bal = (model_bal.predict(X_bal_test) > 0.5).astype("int32")
acc_bal = accuracy_score(y_bal_test, pred_bal)

# ============================================================
# PRINT RESULTS
# ============================================================

print("\n============================")
print(" NEURAL NETWORK COMPARISON")
print("============================")
print(f"Without Optimizer (SGD) Accuracy:   {no_opt_acc:.4f}")
print("Precision:", no_opt_precision)
print("Recall   :", no_opt_recall)
print("F1 Score :", no_opt_f1)
print(f"With Adam Optimizer Accuracy:     {nn_acc:.4f}")
print("Precision:", nn_precision)
print("Recall   :", nn_recall)
print("F1 Score :", nn_f1)

print("\n============================")
print(" IMBALANCED VS BALANCED DATA")
print("============================")
print(f"Imbalanced Accuracy: {acc_imp:.4f}")
print(f"Balanced (SMOTE) Accuracy: {acc_bal:.4f}")

""" Algorithm Selection & Hyperparameter Tuning"""

#  RANDOM FOREST (GRID SEARCH TUNING)

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [4, 6, 8, None],
    'min_samples_split': [2, 5, 10]
}
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_bal_res, y_bal_res)
best_rf = grid_rf.best_estimator_
y_pred_rf = best_rf.predict(X_bal_test)

rf_acc = accuracy_score(y_bal_test, y_pred_rf)
rf_prec = precision_score(y_bal_test, y_pred_rf)
rf_rec = recall_score(y_bal_test, y_pred_rf)
rf_f1 = f1_score(y_bal_test, y_pred_rf)

print("\n--- Random Forest ---")
print("Best Params:", grid_rf.best_params_)
print(f"Accuracy: {rf_acc:.4f}, Precision: {rf_prec:.4f}, Recall: {rf_rec:.4f}, F1: {rf_f1:.4f}")

""" Confusion Matrix"""

cm = confusion_matrix(y_bal_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from imblearn.pipeline import Pipeline as ImbPipeline


#  LOGISTIC REGRESSION


smote = SMOTE(random_state=42)

log_pipe = ImbPipeline([
    ("scaler", StandardScaler()),
    ("smote", smote),
    ("clf", LogisticRegression(max_iter=5000, class_weight="balanced"))
])

param_grid_lr = {
    "clf__C": [0.01, 0.1, 1, 10],
    "clf__solver": ["liblinear", "lbfgs"],
    "clf__class_weight": ["balanced"]

}

log_grid = GridSearchCV(
    log_pipe,
    param_grid=param_grid_lr,
    cv=5,
    scoring="f1",
    n_jobs=-1
)

log_grid.fit(X_bal_train, y_bal_train)
best_log = log_grid.best_estimator_
y_pred_log = best_log.predict(X_bal_test)

log_acc = accuracy_score(y_bal_test, y_pred_log)
log_prec = precision_score(y_bal_test, y_pred_log)
log_rec = recall_score(y_bal_test, y_pred_log)
log_f1 = f1_score(y_bal_test, y_pred_log)

print("\n--- Tuned Logistic Regression ---")
print("Best Params:", log_grid.best_params_)
print(f"Accuracy : {log_acc:.4f}")
print(f"Precision: {log_prec:.4f}")
print(f"Recall   : {log_rec:.4f}")
print(f"F1 Score : {log_f1:.4f}")

#  SUPPORT VECTOR MACHINE

svm_pipe = ImbPipeline([
    ("scaler", StandardScaler()),
    ("smote", smote),
    ("clf", SVC(probability=True, class_weight="balanced"))
])

svm_param_grid = {
    "clf__C": [0.1, 1, 10],
    "clf__kernel": ["linear", "rbf"],
    "clf__gamma": ["scale", "auto"]
}

svm_grid = GridSearchCV(
    svm_pipe,
    param_grid=svm_param_grid,
    cv=5,
    scoring="f1",
    n_jobs=-1
)

svm_grid.fit(X_bal_train, y_bal_train)
best_svm = svm_grid.best_estimator_
y_pred_svm = best_svm.predict(X_bal_test)

svm_acc = accuracy_score(y_bal_test, y_pred_svm)
svm_prec = precision_score(y_bal_test, y_pred_svm)
svm_rec = recall_score(y_bal_test, y_pred_svm)
svm_f1 = f1_score(y_bal_test, y_pred_svm)

print("\n--- Tuned SVM ---")
print("Best Params:", svm_grid.best_params_)
print(f"Accuracy : {svm_acc:.4f}")
print(f"Precision: {svm_prec:.4f}")
print(f"Recall   : {svm_rec:.4f}")
print(f"F1 Score : {svm_f1:.4f}")

# MODEL COMPARISON AND VISUALIZATION

results = pd.DataFrame({
    'Model': ['Neural Network (Adam)', 'Random Forest', 'Logistic Regression', 'SVM'],
    'Accuracy': [nn_acc, rf_acc, log_acc, svm_acc],
    'Precision': [nn_precision, rf_prec, log_prec, svm_prec],
    'Recall': [nn_recall, rf_rec, log_rec, svm_rec],
    'F1 Score': [nn_f1, rf_f1, log_f1, svm_f1]
})
print("\nModel Comparison:\n", results)

# MODEL COMPARISON AND VISUALIZATION

results = pd.DataFrame({
    'Model': ['Neural Network (Adam)', 'Random Forest', 'Logistic Regression', 'SVM'],
    'Accuracy': [nn_acc, rf_acc, log_acc, svm_acc],
    'Precision': [nn_precision, rf_prec, log_prec, svm_prec],
    'Recall': [nn_recall, rf_rec, log_rec, svm_rec],
    'F1 Score': [nn_f1, rf_f1, log_f1, svm_f1]
})
print("\nModel Comparison:\n", results)

# Plot Accuracy Comparison
plt.figure(figsize=(10,7))
colors = ['#9467bd', '#2ca02c', '#1f77b4', '#d62728']
bars = plt.bar(results['Model'], results['Accuracy'], color=colors)
plt.title('Model Accuracy Comparison', fontsize=13)
plt.xlabel('Model', fontsize=11)
plt.ylabel('Accuracy', fontsize=11)
plt.ylim(0, 1.1)

for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.01, f"{bar.get_height():.3f}",
             ha='center', va='bottom', fontsize=10, fontweight='bold')
plt.show()